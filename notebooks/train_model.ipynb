{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-10-03T19:28:40.803679Z",
     "iopub.status.busy": "2024-10-03T19:28:40.803333Z",
     "iopub.status.idle": "2024-10-03T19:28:42.043446Z",
     "shell.execute_reply": "2024-10-03T19:28:42.042229Z"
    },
    "papermill": {
     "duration": 1.2501,
     "end_time": "2024-10-03T19:28:42.046535",
     "exception": false,
     "start_time": "2024-10-03T19:28:40.796435",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "changed dir to /Users/farooq.azam/SAP_Related_Code/predictive-ai-starter)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import datarobot as dr # type: ignore\n",
    "from dotenv import load_dotenv # type: ignore\n",
    "\n",
    "# The notebook should be executed from the project root directory\n",
    "if \"_correct_path\" not in locals():\n",
    "    os.chdir(\"..\")\n",
    "    sys.path.append(\".\")\n",
    "    print(f\"changed dir to {Path('.').resolve()})\")\n",
    "    _correct_path = True\n",
    "load_dotenv()\n",
    "client = dr.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8b72fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client version: 3.6.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Client version:\", dr.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9526930",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from datarobot.errors import DataRobotDeprecationWarning # type: ignore\n",
    "warnings.filterwarnings('ignore', category=DataRobotDeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b161892ca68b251c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T19:28:42.066548Z",
     "iopub.status.busy": "2024-10-03T19:28:42.065898Z",
     "iopub.status.idle": "2024-10-03T19:28:43.659849Z",
     "shell.execute_reply": "2024-10-03T19:28:43.658490Z"
    },
    "papermill": {
     "duration": 1.605124,
     "end_time": "2024-10-03T19:28:43.662329",
     "exception": false,
     "start_time": "2024-10-03T19:28:42.057205",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use case Recipe Template Use Case [LPM_FA_V22] created with id 677095535fe1ddf91d6057b6\n"
     ]
    }
   ],
   "source": [
    "from datarobotx.idp.use_cases import get_or_create_use_case # type: ignore\n",
    "from infra.settings_main import use_case_args\n",
    "\n",
    "if \"DATAROBOT_DEFAULT_USE_CASE\" in os.environ:\n",
    "    use_case_id = os.environ[\"DATAROBOT_DEFAULT_USE_CASE\"]\n",
    "else:\n",
    "    use_case_id = get_or_create_use_case(\n",
    "        endpoint=client.endpoint,\n",
    "        token=client.token,\n",
    "        name=use_case_args.resource_name,\n",
    "        description=use_case_args.description,\n",
    "    )\n",
    "    print(f\"Use case {use_case_args.resource_name} created with id {use_case_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d111948bcc943419",
   "metadata": {
    "papermill": {
     "duration": 0.004064,
     "end_time": "2024-10-03T19:28:43.672062",
     "exception": false,
     "start_time": "2024-10-03T19:28:43.667998",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data Ingest and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8611f861f1224dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T19:28:43.682281Z",
     "iopub.status.busy": "2024-10-03T19:28:43.681668Z",
     "iopub.status.idle": "2024-10-03T19:28:43.705618Z",
     "shell.execute_reply": "2024-10-03T19:28:43.704793Z"
    },
    "papermill": {
     "duration": 0.030906,
     "end_time": "2024-10-03T19:28:43.707654",
     "exception": false,
     "start_time": "2024-10-03T19:28:43.676748",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAP Credentials ID - 6747707642a92d74cd796960\n",
      "SAP Credentials - Credential('6747707642a92d74cd796960', 'DR_SAP_TEMPLATE_CRED', 'basic')\n",
      "SAP Datastore ID - 676c46ce57b0ab717e45149a\n",
      "SAP Datastore - DataStore('DR_SAP_TEMPLATE [72d5954]')\n",
      "SAP Datasource ID - 676c46d257b0ab717e45149b\n",
      "SAP Datasource - DataSource('LATE_PAYMENTS_TRAINING_DATA_DSP [4f747c4]')\n",
      "SAP Training data set ID - 676cad9bfc42109cd1340d6c\n",
      "SAP Training data set - Dataset(name='DRS_LATE_PAYMENTS_TRAINING_DATA_VIEW_DSP [3092c80]', id='676cad9bfc42109cd1340d6c')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd # type: ignore\n",
    "from infra.settings_datasets import training_dataset\n",
    "from datarobotx.idp.datasets import get_or_create_dataset_from_datasource # type: ignore\n",
    "from datarobotx.idp.datasource import get_or_create_datasource # type: ignore\n",
    "from datarobotx.idp.datastore import get_or_create_datastore # type: ignore\n",
    "from datarobotx.idp.credentials import get_replace_or_create_credential # type: ignore\n",
    "\n",
    "\n",
    "sap_dsp_data_store_canonical_name=os.getenv(\"SAP_DSP_DATA_STORE_CANONICAL_NAME\")\n",
    "sap_dsp_host_name=os.getenv(\"SAP_DSP_HOST_NAME\")\n",
    "sap_dsp_port=os.getenv(\"SAP_DSP_PORT\")\n",
    "sap_dsp_data_source=os.getenv(\"SAP_DSP_DATA_SOURCE\") \n",
    "sap_dsp_data_query=os.getenv(\"SAP_DSP_LATE_PAYMENTS_DATA_QUERY\")\n",
    "sap_dsp_training_data_set=os.getenv(\"SAP_DSP_LATE_PAYMENTS_TRAINING_DATA_SET\")\n",
    "sap_dsp_credentials=os.getenv(\"SAP_DSP_CREDENTIALS\")\n",
    "\n",
    "sap_dsp_credentials_id = get_replace_or_create_credential(\n",
    "    endpoint=client.endpoint,\n",
    "    token=client.token,\n",
    "    name=sap_dsp_credentials,\n",
    "    credential_type=\"basic\",\n",
    ")\n",
    "print(f\"SAP Credentials ID - {sap_dsp_credentials_id}\")\n",
    "sap_dsp_credentials = dr.Credential.get(sap_dsp_credentials_id)\n",
    "print(f\"SAP Credentials - {sap_dsp_credentials}\")\n",
    "sap_dsp_data_store_id=get_or_create_datastore(\n",
    "    endpoint=client.endpoint,\n",
    "    token=client.token,\n",
    "    canonical_name=sap_dsp_data_store_canonical_name,\n",
    "    driver_id='66c8ecdd45d2b5465fd74b49',\n",
    "    data_store_type='dr-database-v1',\n",
    "    fields=[{\"id\":\"host\",\"name\":\"Host Name\",\"value\":sap_dsp_host_name},{\"id\":\"port\",\"name\":\"port\",\"value\":sap_dsp_port}],\n",
    ")\n",
    "sap_dsp_data_store=dr.DataStore.get(sap_dsp_data_store_id)\n",
    "print(f\"SAP Datastore ID - {sap_dsp_data_store_id}\")\n",
    "print(f\"SAP Datastore - {sap_dsp_data_store}\")\n",
    "params = dr.DataSourceParameters(\n",
    "    data_store_id=sap_dsp_data_store.id,\n",
    "    query=sap_dsp_data_query,\n",
    ")\n",
    "sap_dsp_data_source_id = get_or_create_datasource(\n",
    "    endpoint=client.endpoint,\n",
    "    token=client.token,\n",
    "    data_source_type='dr-database-v1', \n",
    "    canonical_name=sap_dsp_data_source,\n",
    "    params=params\n",
    ")\n",
    "print(f\"SAP Datasource ID - {sap_dsp_data_source_id}\")\n",
    "print(f\"SAP Datasource - {dr.DataSource.get(sap_dsp_data_source_id)}\")\n",
    "training_dataset_id=get_or_create_dataset_from_datasource(\n",
    "    endpoint=client.endpoint,\n",
    "    token=client.token,\n",
    "    data_source_id=sap_dsp_data_source_id,\n",
    "    name=sap_dsp_training_data_set,\n",
    "    credential_id=sap_dsp_credentials.credential_id\n",
    ")\n",
    "print(f\"SAP Training data set ID - {training_dataset_id}\")\n",
    "print(f\"SAP Training data set - {dr.Dataset.get(training_dataset_id)}\")\n",
    "training_dataset=dr.Dataset.get(training_dataset_id)\n",
    "training_dataset.training_data_id=training_dataset_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1889058273e97b",
   "metadata": {
    "papermill": {
     "duration": 0.004072,
     "end_time": "2024-10-03T19:28:45.367695",
     "exception": false,
     "start_time": "2024-10-03T19:28:45.363623",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49d9f03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_or_create_featurelist(dataset: dr.Dataset, name: str, features: list[str]) -> str:\n",
    "    try:\n",
    "        feature_lists = dataset.get_featurelists()\n",
    "        for feature_list in feature_lists:\n",
    "            if feature_list.name == name:   \n",
    "                return feature_list.id\n",
    "        raise dr.errors.ClientError(\"Could not find featurelist!\",404)\n",
    "    except dr.errors.ClientError:\n",
    "        print(f\"Creating featurelist {name}\")\n",
    "        featurelist = dataset.create_featurelist(name=name, features=features)\n",
    "    return featurelist.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2735fb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "self_join_features = [\n",
    "    \"Days_Late\", #target, invoice due date to actual paid date\n",
    "    \"Days_to_Ship\", #historical information, order date to ship date\n",
    "    \"ORDER_DATE\",\n",
    "    \"INVOICE_DUE_DATE\",\n",
    "    \"Payment_Status\", #late, on time, early\n",
    "    \"EXPECTED_AMOUNT\",\n",
    "    \"PAYMENT_TERM\",\n",
    "    \"CUSTOMER_NAME\",\n",
    "    \"ACTUAL_INVOICED_QUANTITY_CASES\",\n",
    "    \"MATERIAL_NAME\", \n",
    "]\n",
    "known_features = [\n",
    "    \"ORDER_DATE\",\n",
    "    \"EXPECTED_AMOUNT\",\n",
    "    \"PAYMENT_TERM\",\n",
    "    \"CUSTOMER_NAME\",\n",
    "    \"ACTUAL_INVOICED_QUANTITY_CASES\",\n",
    "    \"MATERIAL_NAME\", \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef9ef127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self join feature list ID: 676cae1ee757893abb576c28\n",
      "Known features list ID: 676cae1e1004162e5a8e54d7\n"
     ]
    }
   ],
   "source": [
    "SELF_JOIN_LIST_NAME = \"self_join_features_list\"\n",
    "KNOWN_FEATURES_LIST_NAME = \"known_features_list\"\n",
    "self_join_features_list_id = get_or_create_featurelist(training_dataset, SELF_JOIN_LIST_NAME, self_join_features)\n",
    "known_features_list_id = get_or_create_featurelist(training_dataset, KNOWN_FEATURES_LIST_NAME, known_features)\n",
    "print(f\"Self join feature list ID: {self_join_features_list_id}\")\n",
    "print(f\"Known features list ID: {known_features_list_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b555df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LPM_FA_V22] Project ID: 6770a228734fd7a7927176e2\n"
     ]
    }
   ],
   "source": [
    "from infra.settings_main import project_name\n",
    "xpa_use_case=dr.UseCase.get(use_case_id)\n",
    "project = dr.Project.create_from_dataset(\n",
    "    dataset_id=training_dataset.id,\n",
    "    project_name=f\"Recipe Template Project [{project_name}]\",\n",
    "    use_case=xpa_use_case\n",
    ")\n",
    "print(f\"[{project_name}] Project ID: {project.id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60ecce6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primary feature list ID: 6770a232bae86891c5751167\n"
     ]
    }
   ],
   "source": [
    "feature_lists_for_project = project.get_featurelists()\n",
    "primary_featurelist = [ flist for flist in feature_lists_for_project if flist.name.find(\"known\") != -1][0]\n",
    "primary_featurelist_id = primary_featurelist.id\n",
    "print(f\"Primary feature list ID: {primary_featurelist_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de41427c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# customer_secondary = dr.DatasetDefinition(\n",
    "customer_secondary_dataset=dr.helpers.feature_discovery.DatasetDefinition(\n",
    "    identifier=\"CUSTOMER_DATASET\", #name of the secondary dataset\n",
    "    catalog_id=training_dataset.id,\n",
    "    catalog_version_id=training_dataset.version_id,\n",
    "    primary_temporal_key=\"ORDER_DATE\",\n",
    "    feature_list_id=self_join_features_list_id,\n",
    "    snapshot_policy = \"latest\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2c78d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the relationship between the primary and secondary datasets\n",
    "#customer_relationship = dr.Relationship(\n",
    "customer_relationship = dr.helpers.feature_discovery.Relationship(\n",
    "    dataset2_identifier=\"CUSTOMER_DATASET\",\n",
    "    dataset1_keys=[\"CUSTOMER_NAME\"],\n",
    "    dataset2_keys=[\"CUSTOMER_NAME\"],\n",
    "    feature_derivation_windows=[\n",
    "        {\"start\": -91, \"end\": -61, \"unit\": \"DAY\"},\n",
    "        {\"start\": -151, \"end\": -61, \"unit\": \"DAY\"},\n",
    "    ],\n",
    "    prediction_point_rounding=1,\n",
    "    prediction_point_rounding_time_unit=\"DAY\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10b62029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the  data set definitions and four relationships into lists\n",
    "datasets_definitions = [customer_secondary_dataset]\n",
    "datasets_relationships = [customer_relationship]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ca049cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the aggregations to be created\n",
    "feature_discovery_settings=[  \n",
    "        {\"name\": \"enable_days_from_prediction_point\", \"value\": True},\n",
    "        {\"name\": \"enable_hour\", \"value\": False},\n",
    "        {\"name\": \"enable_categorical_num_unique\", \"value\": True},\n",
    "        {\"name\": \"enable_categorical_statistics\", \"value\": True},\n",
    "        {\"name\": \"enable_numeric_minimum\", \"value\": True},\n",
    "        {\"name\": \"enable_token_counts\", \"value\": False},\n",
    "        {\"name\": \"enable_latest_value\", \"value\": True},\n",
    "        {\"name\": \"enable_numeric_standard_deviation\", \"value\": True},\n",
    "        {\"name\": \"enable_numeric_skewness\", \"value\": True},\n",
    "        {\"name\": \"enable_day_of_week\", \"value\": True},\n",
    "        {\"name\": \"enable_entropy\", \"value\": True},\n",
    "        {\"name\": \"enable_numeric_median\", \"value\": True},\n",
    "        {\"name\": \"enable_word_count\", \"value\": False},\n",
    "        {\"name\": \"enable_pairwise_time_difference\", \"value\": True},\n",
    "        {\"name\": \"enable_days_since_previous_event\", \"value\": True},\n",
    "        {\"name\": \"enable_numeric_maximum\", \"value\": True},\n",
    "        {\"name\": \"enable_numeric_kurtosis\", \"value\": True},\n",
    "        {\"name\": \"enable_most_frequent\", \"value\": True},\n",
    "        {\"name\": \"enable_day\", \"value\": True},\n",
    "        {\"name\": \"enable_numeric_average\", \"value\": True},\n",
    "        {\"name\": \"enable_summarized_counts\", \"value\": True},\n",
    "        {\"name\": \"enable_missing_count\", \"value\": False},\n",
    "        {\"name\": \"enable_record_count\", \"value\": True},\n",
    "        {\"name\": \"enable_numeric_sum\", \"value\": True},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "560f3516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relationship configuration ID: 6770a25ad556b7e6126057dd\n"
     ]
    }
   ],
   "source": [
    "# Create the relationships configuration to define connection between the datasets\n",
    "#relationship_config=dr.RelationshipsConfiguration.create(\n",
    "relationship_config = dr.RelationshipsConfiguration.create(\n",
    "    dataset_definitions=datasets_definitions,\n",
    "    relationships=datasets_relationships,\n",
    "    feature_discovery_settings=feature_discovery_settings,\n",
    ")\n",
    "print(f\"Relationship configuration ID: {relationship_config.id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d110df13594623e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T19:28:45.398923Z",
     "iopub.status.busy": "2024-10-03T19:28:45.398434Z",
     "iopub.status.idle": "2024-10-03T19:28:49.132597Z",
     "shell.execute_reply": "2024-10-03T19:28:49.131627Z"
    },
    "papermill": {
     "duration": 3.741162,
     "end_time": "2024-10-03T19:28:49.133955",
     "exception": true,
     "start_time": "2024-10-03T19:28:45.392793",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Autopilot...\n",
      "In progress: 15, queued: 0 (waited: 0s)\n",
      "In progress: 15, queued: 0 (waited: 1s)\n",
      "In progress: 15, queued: 0 (waited: 2s)\n",
      "In progress: 15, queued: 0 (waited: 3s)\n",
      "In progress: 15, queued: 0 (waited: 4s)\n",
      "In progress: 15, queued: 0 (waited: 6s)\n",
      "In progress: 15, queued: 0 (waited: 10s)\n",
      "In progress: 15, queued: 0 (waited: 18s)\n",
      "In progress: 15, queued: 0 (waited: 31s)\n",
      "In progress: 15, queued: 0 (waited: 52s)\n",
      "In progress: 15, queued: 0 (waited: 73s)\n",
      "In progress: 15, queued: 0 (waited: 93s)\n",
      "In progress: 10, queued: 0 (waited: 114s)\n",
      "In progress: 9, queued: 0 (waited: 135s)\n",
      "In progress: 9, queued: 0 (waited: 155s)\n",
      "In progress: 9, queued: 0 (waited: 176s)\n",
      "In progress: 8, queued: 0 (waited: 197s)\n",
      "In progress: 8, queued: 0 (waited: 218s)\n",
      "In progress: 8, queued: 0 (waited: 238s)\n",
      "In progress: 7, queued: 0 (waited: 259s)\n",
      "In progress: 6, queued: 0 (waited: 280s)\n",
      "In progress: 5, queued: 0 (waited: 301s)\n",
      "In progress: 5, queued: 0 (waited: 321s)\n",
      "In progress: 5, queued: 0 (waited: 342s)\n",
      "In progress: 5, queued: 0 (waited: 363s)\n",
      "In progress: 5, queued: 0 (waited: 383s)\n",
      "In progress: 5, queued: 0 (waited: 404s)\n",
      "In progress: 5, queued: 0 (waited: 425s)\n",
      "In progress: 5, queued: 0 (waited: 446s)\n",
      "In progress: 5, queued: 0 (waited: 466s)\n",
      "In progress: 4, queued: 0 (waited: 487s)\n",
      "In progress: 4, queued: 0 (waited: 508s)\n",
      "In progress: 3, queued: 0 (waited: 528s)\n",
      "In progress: 3, queued: 0 (waited: 549s)\n",
      "In progress: 3, queued: 0 (waited: 570s)\n",
      "In progress: 3, queued: 0 (waited: 590s)\n",
      "In progress: 3, queued: 0 (waited: 611s)\n",
      "In progress: 3, queued: 0 (waited: 632s)\n",
      "In progress: 3, queued: 0 (waited: 653s)\n",
      "In progress: 3, queued: 0 (waited: 673s)\n",
      "In progress: 3, queued: 0 (waited: 694s)\n",
      "In progress: 3, queued: 0 (waited: 715s)\n",
      "In progress: 3, queued: 0 (waited: 735s)\n",
      "In progress: 1, queued: 0 (waited: 756s)\n",
      "In progress: 1, queued: 0 (waited: 777s)\n",
      "In progress: 1, queued: 0 (waited: 798s)\n",
      "In progress: 1, queued: 0 (waited: 818s)\n",
      "In progress: 1, queued: 0 (waited: 839s)\n",
      "In progress: 1, queued: 0 (waited: 860s)\n",
      "In progress: 1, queued: 0 (waited: 880s)\n",
      "In progress: 1, queued: 0 (waited: 901s)\n",
      "In progress: 0, queued: 0 (waited: 922s)\n",
      "In progress: 0, queued: 0 (waited: 942s)\n",
      "In progress: 0, queued: 0 (waited: 963s)\n",
      "In progress: 0, queued: 0 (waited: 984s)\n",
      "In progress: 0, queued: 0 (waited: 1004s)\n",
      "In progress: 0, queued: 0 (waited: 1025s)\n",
      "In progress: 8, queued: 1 (waited: 1046s)\n",
      "In progress: 8, queued: 1 (waited: 1067s)\n",
      "In progress: 8, queued: 1 (waited: 1087s)\n",
      "In progress: 4, queued: 1 (waited: 1108s)\n",
      "In progress: 4, queued: 1 (waited: 1129s)\n",
      "In progress: 3, queued: 1 (waited: 1149s)\n",
      "In progress: 3, queued: 1 (waited: 1170s)\n",
      "In progress: 2, queued: 1 (waited: 1191s)\n",
      "In progress: 1, queued: 1 (waited: 1212s)\n",
      "In progress: 1, queued: 0 (waited: 1232s)\n",
      "In progress: 1, queued: 0 (waited: 1254s)\n",
      "In progress: 1, queued: 0 (waited: 1274s)\n",
      "In progress: 1, queued: 0 (waited: 1295s)\n",
      "In progress: 1, queued: 0 (waited: 1316s)\n",
      "In progress: 1, queued: 0 (waited: 1336s)\n",
      "In progress: 1, queued: 0 (waited: 1357s)\n",
      "In progress: 1, queued: 0 (waited: 1378s)\n",
      "In progress: 1, queued: 0 (waited: 1399s)\n",
      "In progress: 1, queued: 0 (waited: 1420s)\n",
      "In progress: 1, queued: 0 (waited: 1441s)\n",
      "In progress: 1, queued: 0 (waited: 1461s)\n",
      "In progress: 1, queued: 0 (waited: 1482s)\n",
      "In progress: 1, queued: 0 (waited: 1503s)\n",
      "In progress: 1, queued: 0 (waited: 1524s)\n",
      "In progress: 1, queued: 0 (waited: 1544s)\n",
      "In progress: 1, queued: 0 (waited: 1565s)\n",
      "In progress: 0, queued: 0 (waited: 1586s)\n",
      "In progress: 0, queued: 0 (waited: 1606s)\n",
      "In progress: 0, queued: 0 (waited: 1627s)\n",
      "In progress: 0, queued: 0 (waited: 1648s)\n",
      "In progress: 0, queued: 0 (waited: 1669s)\n",
      "In progress: 0, queued: 0 (waited: 1689s)\n",
      "Recommended model ID: 6770aa44fedc2a27a4a2cf39\n",
      "Registered recommended model...\n",
      "Registered model version ID: 6770acd995fcb092c26057b7\n"
     ]
    }
   ],
   "source": [
    "from datarobotx.idp.autopilot import get_or_create_autopilot_run # type: ignore\n",
    "from datarobotx.idp.registered_model_versions import ( # type: ignore\n",
    "    get_or_create_registered_leaderboard_model_version,\n",
    ")\n",
    "advanced_options = dr.AdvancedOptions(\n",
    "    blend_best_models=False, \n",
    "    prepare_model_for_deployment=True,\n",
    "    seed=0,\n",
    ")\n",
    "datetime_partition_spec = dr.DatetimePartitioningSpecification(\n",
    "    datetime_partition_column=\"ORDER_DATE\",\n",
    "    use_time_series=False,  # This is not a time series project\n",
    "    disable_holdout=False,\n",
    "    holdout_start_date=pd.to_datetime(\"2023-12-01\",format='%Y-%m-%d'),  # Adjust based on your data\n",
    "    holdout_end_date=pd.to_datetime(\"2024-05-01\",format='%Y-%m-%d') ,\n",
    "    #validation_start_date=\"2023-12-01\",  # Adjust based on your data\n",
    "    validation_duration=\"P6M\",\n",
    "    #gap=61,  # Matching your feature derivation window gap\n",
    "    number_of_backtests=2,\n",
    ")\n",
    "\n",
    "print(\"Running Autopilot...\")\n",
    "project.analyze_and_model(\n",
    "    target=\"Days_Late\",\n",
    "    mode=dr.AUTOPILOT_MODE.QUICK,\n",
    "    featurelist_id=primary_featurelist_id,\n",
    "    relationships_configuration_id=relationship_config.id,\n",
    "    feature_engineering_prediction_point=\"ORDER_DATE\",\n",
    "    partitioning_method=datetime_partition_spec,\n",
    "    advanced_options=advanced_options,\n",
    "    worker_count=-1, \n",
    "    max_wait=6000 * 6000 * 6000,\n",
    " )\n",
    "\n",
    "project.wait_for_autopilot()\n",
    "model_id = dr.ModelRecommendation.get(project.id).model_id\n",
    "print(f\"Recommended model ID: {model_id}\")\n",
    "registered_model_name = f\"Recipe Template Registered Model [{project_name}]\"\n",
    "print(\"Registered recommended model...\")\n",
    "registered_model_version_id = get_or_create_registered_leaderboard_model_version(\n",
    "    endpoint=client.endpoint,\n",
    "    token=client.token,\n",
    "    model_id=model_id,\n",
    "    registered_model_name=registered_model_name,\n",
    ")\n",
    "print(f\"Registered model version ID: {registered_model_version_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea44739b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datarobotx.idp.deployments import ( # type: ignore\n",
    "    _lookup_registered_model_version,\n",
    "    get_or_create_deployment_from_registered_model_version,\n",
    "    get_replace_or_create_deployment_from_registered_model,\n",
    ")\n",
    "from infra.settings_main import model_training_output_path\n",
    "import yaml # type: ignore\n",
    "\n",
    "dr.PredictionServer.list()\n",
    "prediction_server = dr.PredictionServer.list()[0]\n",
    "deployment_name = f\"Recipe Template Deployment [{project_name}]\"\n",
    "print(\"Creating deployment...\")\n",
    "deployment_id = get_or_create_deployment_from_registered_model_version(\n",
    "    endpoint=client.endpoint,\n",
    "    token=client.token,\n",
    "    label=deployment_name,\n",
    "    registered_model_version_id=registered_model_version_id,\n",
    "    default_prediction_server_id=prediction_server.id,\n",
    ")\n",
    "print(f\"Deployment ID: {deployment_id}\")\n",
    "\n",
    "scoring_related_data_file = \"scoring_related_info.yaml\"\n",
    "scoring_related_data = {\n",
    "    \"use_case_id\": use_case_id,\n",
    "    \"model_id\": model_id,\n",
    "    \"project_id\": project.id,\n",
    "    \"deployment_id\": deployment_id,\n",
    "    \"registered_model_version_id\": registered_model_version_id\n",
    "    }\n",
    "# wrtie to file for later use\n",
    "with open(scoring_related_data_file, \"w\") as file:\n",
    "    yaml.dump(scoring_related_data, file)\n",
    "print(f\"Saved scoring related information to {scoring_related_data_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "eaa0dc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = project.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "26f6fe1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Capturing settings required to deploy the frontend...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'frontend/train_model_output.LPM_FA_V22.yaml'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 28\u001b[0m\n\u001b[1;32m      8\u001b[0m registered_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\n\u001b[1;32m      9\u001b[0m     rm\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m rm \u001b[38;5;129;01min\u001b[39;00m dr\u001b[38;5;241m.\u001b[39mRegisteredModel\u001b[38;5;241m.\u001b[39mlist(search\u001b[38;5;241m=\u001b[39mregistered_model_name)\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m rm\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m registered_model_name\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     14\u001b[0m app_settings \u001b[38;5;241m=\u001b[39m AppSettings(\n\u001b[1;32m     15\u001b[0m     registered_model_version_id\u001b[38;5;241m=\u001b[39mregistered_model_version_id,\n\u001b[1;32m     16\u001b[0m     registered_model_name\u001b[38;5;241m=\u001b[39mregistered_model_name,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m     ),\n\u001b[1;32m     26\u001b[0m )\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel_training_output_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     29\u001b[0m     yaml\u001b[38;5;241m.\u001b[39mdump(app_settings\u001b[38;5;241m.\u001b[39mmodel_dump(), f)\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'frontend/train_model_output.LPM_FA_V22.yaml'"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "from infra.settings_main import model_training_output_path\n",
    "from starter.i18n import gettext\n",
    "from starter.schema import AppSettings\n",
    "\n",
    "print(\"Capturing settings required to deploy the frontend...\")\n",
    "registered_model = next(\n",
    "    rm\n",
    "    for rm in dr.RegisteredModel.list(search=registered_model_name)\n",
    "    if rm.name == registered_model_name\n",
    ")\n",
    "\n",
    "app_settings = AppSettings(\n",
    "    registered_model_version_id=registered_model_version_id,\n",
    "    registered_model_name=registered_model_name,\n",
    "    use_case_id=use_case_id,\n",
    "    project_id=project.id,\n",
    "    model_id=model_id,\n",
    "    target=target,\n",
    "    training_dataset_id=training_dataset_id,\n",
    "    page_title=gettext(\"Predictive AI Starter\"),\n",
    "    page_description=gettext(\n",
    "        \"An application designed to simplify interactions with predictions while providing clear insights into the key drivers behind those predictions.\"\n",
    "    ),\n",
    ")\n",
    "with open(model_training_output_path, \"w\") as f:\n",
    "    yaml.dump(app_settings.model_dump(), f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 10.026671,
   "end_time": "2024-10-03T19:28:49.662019",
   "environment_variables": {},
   "exception": true,
   "input_path": "notebooks/train_model.ipynb",
   "output_path": "notebooks/train_model.ipynb",
   "parameters": {},
   "start_time": "2024-10-03T19:28:39.635348",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
